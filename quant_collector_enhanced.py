# =========================================================
# quant_collector.py  â€”  í•œêµ­ ì£¼ì‹ í€€íŠ¸ ë°ì´í„° ìˆ˜ì§‘ê¸°
# ---------------------------------------------------------
# ìˆ˜ì§‘ í•­ëª©:
#   1) ì¢…ëª© ë§ˆìŠ¤í„° (KRX ì „ì¢…ëª©)
#   2) ì¼ë³„ ì‹œì„¸ + í€ë”ë©˜í„¸ (ì¢…ê°€, ì‹œê°€ì´ì•¡, EPS, BPS, DPS)
#   3) ì¬ë¬´ì œí‘œ (IS / BS / CF â€” ì—°ê°„ + ë¶„ê¸°)
#   4) í•µì‹¬ ì§€í‘œ (Financial Highlight + ì¬ë¬´ë¹„ìœ¨)
#   5) ì£¼ì‹ìˆ˜ (ë°œí–‰ì£¼ì‹ìˆ˜, ìì‚¬ì£¼, ìœ í†µì£¼ì‹ìˆ˜)
#
# ì¶œë ¥: ./data/ í´ë”ì— CSV íŒŒì¼ë¡œ ì €ì¥ (DB ë¶ˆí•„ìš”)
# 
# ì‹¤í–‰:
#   í…ŒìŠ¤íŠ¸ ëª¨ë“œ: python quant_collector.py --test
#   ì „ì²´ ì‹¤í–‰:   python quant_collector.py
# =========================================================

import os
import re
import sys
import logging
import warnings
import argparse
from datetime import datetime, date, timedelta  # timedelta ì¶”ê°€
from io import StringIO
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import pandas as pd
import requests
import FinanceDataReader as fdr
from pykrx import stock
from tqdm import tqdm

warnings.filterwarnings("ignore")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)
log = logging.getLogger("COLLECTOR")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR = Path("./data")
DATA_DIR.mkdir(exist_ok=True)

MAX_WORKERS = 15          # FnGuide ë™ì‹œ ìš”ì²­ ìˆ˜ (ë„ˆë¬´ ë†’ìœ¼ë©´ ì°¨ë‹¨ë¨)
REQUEST_TIMEOUT = 12      # ì´ˆ

# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì¢…ëª© (ëŒ€í‘œ ì¢…ëª© ì„ ì •)
TEST_TICKERS = [
    "005930",  # ì‚¼ì„±ì „ì
    "035720",  # ì¹´ì¹´ì˜¤
    "000660",  # SKí•˜ì´ë‹‰ìŠ¤
]

# ì „ì—­ ì„¸ì…˜ (TCP ì»¤ë„¥ì…˜ ì¬ì‚¬ìš©)
_session = requests.Session()
_session.headers.update({
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    )
})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê³µí†µ ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_biz_day() -> str:
    """ìµœê·¼ ì˜ì—…ì¼ (YYYYMMDD) - ì„œë²„ í†µì‹  ì—†ì´ ë¡œì»¬ ê³„ì‚°"""
    d = datetime.now()
    # ì›”=0 ... ê¸ˆ=4, í† =5, ì¼=6
    if d.weekday() == 5:    # í† ìš”ì¼ì´ë©´
        d = d - timedelta(days=1)  # ê¸ˆìš”ì¼ë¡œ
    elif d.weekday() == 6:  # ì¼ìš”ì¼ì´ë©´
        d = d - timedelta(days=2)  # ê¸ˆìš”ì¼ë¡œ
    
    # í‰ì¼ ì˜¤ì „ 9ì‹œ ì´ì „ì´ë©´(ì¥ ì‹œì‘ ì „), ì „ë‚  ë°ì´í„°ë¥¼ ë³´ê¸° ìœ„í•´ í•˜ë£¨ ëºŒ (ì„ íƒì‚¬í•­, ì¼ë‹¨ì€ ë‹¹ì¼ ê¸°ì¤€)
    return d.strftime("%Y%m%d")


def load_tables(url: str) -> list:
    """FnGuide HTML í…Œì´ë¸” íŒŒì‹± (ì¸ì½”ë”© ìë™ ê°ì§€)"""
    try:
        r = _session.get(url, timeout=REQUEST_TIMEOUT)
        r.raise_for_status()
    except Exception:
        return []

    for enc in ("cp949", "euc-kr", "utf-8"):
        try:
            html = r.content.decode(enc, errors="strict")
            return pd.read_html(StringIO(html), displayed_only=False)
        except Exception:
            continue
    # fallback
    try:
        html = r.content.decode("cp949", errors="replace")
        return pd.read_html(StringIO(html), displayed_only=False)
    except Exception:
        return []


def safe_float(x):
    """ì•ˆì „í•œ float ë³€í™˜"""
    if x is None:
        return None
    if isinstance(x, float) and (np.isnan(x) or np.isinf(x)):
        return None
    try:
        s = str(x).replace(",", "").strip()
        if s in ("", "-", "N/A", "nan", "None"):
            return None
        return float(s)
    except (ValueError, TypeError):
        return None


def safe_int(x):
    v = safe_float(x)
    return int(v) if v is not None else None


def parse_period(col_name: str):
    """ì»¬ëŸ¼ëª…ì—ì„œ ê¸°ì¤€ì¼ íŒŒì‹± (2023/12, 2024.03 ë“±)"""
    s = str(col_name)
    is_estimate = "(E)" in s
    m = re.search(r"(\d{4})[\./](\d{2})", s)
    if not m:
        return None, is_estimate
    d = pd.to_datetime(f"{m.group(1)}-{m.group(2)}") + pd.offsets.MonthEnd()
    return d, is_estimate


def normalize_market(m: str) -> str:
    if not m:
        return "ETC"
    m = m.upper()
    if "KOSPI" in m:
        return "KOSPI"
    if "KOSDAQ" in m:
        return "KOSDAQ"
    if "KONEX" in m:
        return "KONEX"
    return "ETC"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. ì¢…ëª© ë§ˆìŠ¤í„°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def collect_master() -> pd.DataFrame:
    """KRX ì „ì¢…ëª© ë§ˆìŠ¤í„° ìˆ˜ì§‘"""
    log.info("ğŸ“˜ ì¢…ëª© ë§ˆìŠ¤í„° ìˆ˜ì§‘ ì¤‘...")
    df = fdr.StockListing("KRX")[["Code", "Name", "Market"]]
    df.columns = ["ì¢…ëª©ì½”ë“œ", "ì¢…ëª©ëª…", "ì‹œì¥êµ¬ë¶„"]
    df["ì‹œì¥êµ¬ë¶„"] = df["ì‹œì¥êµ¬ë¶„"].apply(normalize_market)

    name = df["ì¢…ëª©ëª…"].fillna("")
    code = df["ì¢…ëª©ì½”ë“œ"].fillna("")
    df["ì¢…ëª©êµ¬ë¶„"] = np.select(
        [name.str.contains("ìŠ¤íŒ©"), code.str[-1] != "0", name.str.endswith("ë¦¬ì¸ ")],
        ["ìŠ¤íŒ©", "ìš°ì„ ì£¼", "ë¦¬ì¸ "],
        default="ë³´í†µì£¼",
    )
    log.info(f"  â†’ ì „ì²´ {len(df)}ê°œ ì¢…ëª© ({df['ì¢…ëª©êµ¬ë¶„'].value_counts().to_dict()})")
    return df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. ì¼ë³„ ì‹œì„¸ + í€ë”ë©˜í„¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def collect_daily(biz_day: str) -> pd.DataFrame:
    """FinanceDataReaderë¥¼ ì´ìš©í•œ ì‹œì„¸ + í€ë”ë©˜í„¸ ìˆ˜ì§‘"""
    # biz_day í¬ë§· ë³€ê²½ (YYYYMMDD -> YYYY-MM-DD) í•„ìš” ì‹œ ë³€í™˜, 
    # í•˜ì§€ë§Œ fdr.StockListing('KRX')ëŠ” 'í˜„ì¬' ê¸°ì¤€ ê°€ì¥ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    # ê³¼ê±° íŠ¹ì •ì¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë ¤ë©´ ë³µì¡í•´ì§€ë¯€ë¡œ, ìŠ¤í¬ë¦¬ë„ˆ ëª©ì ìƒ 'ìµœì‹ ' ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
    
    log.info(f"ğŸ“Š ì¼ë³„ ì‹œì„¸ ìˆ˜ì§‘ (ìµœì‹  ê¸°ì¤€)...")

    # 1. KRX ì „ì¢…ëª© ë¦¬ìŠ¤íŒ… (ê°€ê²©, ì‹œê°€ì´ì•¡, ê±°ë˜ëŸ‰ ë“± í¬í•¨ë¨)
    # fdr.StockListing('KRX')ëŠ” ì¢…ê°€, ì‹œê°€ì´ì•¡, ê±°ë˜ëŸ‰ ë“±ì„ ê¸°ë³¸ í¬í•¨í•©ë‹ˆë‹¤.
    df_krx = fdr.StockListing('KRX')
    
    # ì»¬ëŸ¼ ì´ë¦„ì´ í•œê¸€/ì˜ë¬¸ í˜¼ìš©ë  ìˆ˜ ìˆì–´ ì •ë¦¬
    # (ìµœì‹  fdr ë²„ì „ì— ë”°ë¼ ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ í™•ì¸ í›„ ë§¤í•‘)
    # ì¼ë°˜ì ì¸ fdr KRX ì»¬ëŸ¼: Code, Name, Close, Marcap, Stocks, Market ...
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ ë° ë¦¬ë„¤ì„
    rename_map = {
        'Code': 'ì¢…ëª©ì½”ë“œ',
        'Name': 'ì¢…ëª©ëª…',
        'Close': 'ì¢…ê°€',
        'Marcap': 'ì‹œê°€ì´ì•¡',
        'Stocks': 'ìƒì¥ì£¼ì‹ìˆ˜' # í•„ìš”í•˜ë‹¤ë©´
    }
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ë³€ê²½
    avail_cols = [c for c in rename_map if c in df_krx.columns]
    df_krx = df_krx[avail_cols].rename(columns=rename_map)
    
    # 2. í€ë”ë©˜í„¸(PER, PBR ë“±)ì€ fdr.StockListing('KRX-DESC') ë“±ì— ì¼ë¶€ ìˆìœ¼ë‚˜, 
    #    ì •í™•í•œ EPS/BPS/DPSëŠ” KRX ì •ë³´ì‹œìŠ¤í…œì—ì„œ ë³„ë„ë¡œ ê¸ì–´ì•¼ í•˜ëŠ”ë° fdrë¡œëŠ” í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆìŒ.
    #    í•˜ì§€ë§Œ ìŠ¤í¬ë¦¬ë„ˆ ë¡œì§ìƒ PER/PBRì€ 'ì¢…ê°€ / EPS' ë“±ìœ¼ë¡œ ì¬ê³„ì‚°í•˜ë¯€ë¡œ 
    #    EPS, BPS ë°ì´í„°ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
    #    
    #    ëŒ€ì•ˆ: pykrxê°€ ì•ˆë˜ë¯€ë¡œ, fdrì˜ 'KRX' ë°ì´í„°ì— ìˆëŠ” PER, PBRì„ ì“°ê±°ë‚˜
    #    FnGuide í¬ë¡¤ë§ ë‹¨ê³„(fetch_indicators)ì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ ë¯¿ê³  ê°€ì•¼ í•©ë‹ˆë‹¤.
    #    
    #    ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ 'ì‹œì„¸(ì¢…ê°€, ì‹œì´)'ëŠ” fdrë¡œ í™•ì‹¤íˆ ì±™ê¸°ê³ ,
    #    EPS/BPS ì»¬ëŸ¼ì€ ë¹„ì›Œë‘” ë’¤ ë‚˜ì¤‘ì— ì±„ìš°ê±°ë‚˜ 0ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    #    (quant_screener.pyì—ì„œ EPS/BPSê°€ ì—†ìœ¼ë©´ PER/PBR ê³„ì‚°ì„ ëª»í•˜ì§€ë§Œ, 
    #     FnGuide ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê±°ê¸°ì„œ ë³´ì™„ ê°€ëŠ¥í•  ìˆ˜ë„ ìˆìŒ. 
    #     í•˜ì§€ë§Œ í˜„ì¬ êµ¬ì¡°ëŠ” daily.csvì— EPS/BPSê°€ ìˆì–´ì•¼ í•¨.)

    # **ì¤‘ìš”**: pykrxê°€ ê³„ì† í„°ì§€ë¯€ë¡œ, ì¼ë‹¨ ì•ˆì •ì ì¸ fdr ë°ì´í„°ë¡œ 'ì¢…ê°€/ì‹œê°€ì´ì•¡'ë§Œì´ë¼ë„ í™•ë³´í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    # EPS, BPS, ë°°ë‹¹ê¸ˆì€ 0 ë˜ëŠ” Noneìœ¼ë¡œ ì±„ì›Œì„œ ì—ëŸ¬ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    
    for c in ["EPS", "BPS", "ì£¼ë‹¹ë°°ë‹¹ê¸ˆ"]:
        df_krx[c] = None
        
    # ê¸°ì¤€ì¼ ì¶”ê°€
    df_krx["ê¸°ì¤€ì¼"] = biz_day
    
    log.info(f"  â†’ {len(df_krx)}ê°œ ì¢…ëª© ì‹œì„¸ ìˆ˜ì§‘ ì™„ë£Œ (fdr ì‚¬ìš©)")
    return df_krx


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ì¬ë¬´ì œí‘œ (FnGuide)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _classify_fs_table(t: pd.DataFrame) -> str | None:
    """í…Œì´ë¸” 1ì—´ í‚¤ì›Œë“œë¡œ IS/BS/CF íŒë³„"""
    if t.shape[0] < 2 or t.shape[1] < 2:
        return None
    text = " ".join(t.iloc[:, 0].astype(str).tolist())
    if ("ë§¤ì¶œì•¡" in text or "ì˜ì—…ìˆ˜ìµ" in text) and "ìì‚°" not in text:
        return "IS"
    if "ìì‚°" in text and "ë¶€ì±„" in text and "ìë³¸" in text:
        return "BS"
    if "ì˜ì—…í™œë™" in text and "íˆ¬ìí™œë™" in text:
        return "CF"
    return None


def _melt_fs(df: pd.DataFrame, ticker: str, freq: str) -> list[dict]:
    """ì¬ë¬´ì œí‘œ í…Œì´ë¸” â†’ ì„¸ë¡œí˜• dict ë¦¬ìŠ¤íŠ¸"""
    if df is None or df.empty:
        return []
    df = df.loc[:, ~df.columns.str.contains("ì „ë…„ë™ê¸°")]
    df = df.rename(columns={df.columns[0]: "ê³„ì •"})
    df["ê³„ì •"] = df["ê³„ì •"].astype(str).str.replace(
        "ê³„ì‚°ì— ì°¸ì—¬í•œ ê³„ì • í¼ì¹˜ê¸°", "", regex=False
    ).str.strip()
    df = df.drop_duplicates("ê³„ì •", keep="first")

    try:
        melted = pd.melt(df, id_vars="ê³„ì •", var_name="ê¸°ê°„", value_name="ê°’")
    except Exception:
        return []

    rows = []
    for _, r in melted.iterrows():
        biz_date, is_est = parse_period(r["ê¸°ê°„"])
        if biz_date is None:
            continue
        val = safe_float(r["ê°’"])
        if val is None:
            continue
        rows.append({
            "ì¢…ëª©ì½”ë“œ": ticker,
            "ê¸°ì¤€ì¼": biz_date,
            "ê³„ì •": r["ê³„ì •"],
            "ì£¼ê¸°": freq,
            "ê°’": val,
            "ì¶”ì •ì¹˜": is_est,
        })
    return rows


def fetch_fs(ticker: str) -> list[dict]:
    """ì¢…ëª© 1ê°œì˜ ì¬ë¬´ì œí‘œ ìˆ˜ì§‘"""
    url = f"https://comp.fnguide.com/SVO2/ASP/SVD_Finance.asp?pGB=1&gicode=A{ticker}"
    tables = load_tables(url)
    if len(tables) < 2:
        return []

    # í…Œì´ë¸” ë¶„ë¥˜: IS/BS/CF ê°ê° ì—°ê°„(y) â†’ ë¶„ê¸°(q) ìˆœì„œë¡œ ì±„ì›€
    slots = {k: {"y": None, "q": None} for k in ("IS", "BS", "CF")}
    for t in tables:
        label = _classify_fs_table(t)
        if label is None:
            continue
        if slots[label]["y"] is None:
            slots[label]["y"] = t
        elif slots[label]["q"] is None:
            slots[label]["q"] = t

    rows = []
    for fs_type in ("IS", "BS", "CF"):
        for freq_key, freq_label in (("y", "y"), ("q", "q")):
            rows += _melt_fs(slots[fs_type][freq_key], ticker, freq_label)
    return rows


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. í•µì‹¬ ì§€í‘œ (Financial Highlight + ì¬ë¬´ë¹„ìœ¨)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _extract_indicator_rows(
    df: pd.DataFrame, ticker: str, source: str
) -> list[dict]:
    """ì§€í‘œ í…Œì´ë¸” â†’ dict ë¦¬ìŠ¤íŠ¸"""
    if df is None or df.empty or df.shape[1] < 2:
        return []

    # [ìˆ˜ì •] MultiIndex ì»¬ëŸ¼(ë‘ ì¤„ ì´ìƒì˜ í—¤ë”) ì²˜ë¦¬
    # í—¤ë”ê°€ ì—¬ëŸ¬ ì¤„ì¼ ê²½ìš°, ê°€ì¥ ë§ˆì§€ë§‰ ì¤„(ë‚ ì§œê°€ ìˆëŠ” ì¤„)ë§Œ ë‚¨ê¸°ê³  í‰íƒ„í™”
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(-1)

    # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ 'ê³„ì •'ìœ¼ë¡œ ì´ë¦„ ë³€ê²½
    df = df.rename(columns={df.columns[0]: "ê³„ì •"})
    
    # 'ê³„ì •' ì»¬ëŸ¼ì´ ë¬¸ìì—´ì¸ì§€ í™•ì¸ ë° ê³µë°± ì œê±°
    df["ê³„ì •"] = df["ê³„ì •"].astype(str).str.strip()

    try:
        melted = pd.melt(df, id_vars="ê³„ì •", var_name="ê¸°ê°„", value_name="ê°’")
    except Exception:
        return []

    rows = []
    for _, r in melted.iterrows():
        biz_date, is_est = parse_period(r["ê¸°ê°„"])
        if biz_date is None:
            continue
        account = str(r["ê³„ì •"]).strip()
        if not account or account.lower() in ("nan", "none"):
            continue
        val = safe_float(r["ê°’"])
        rows.append({
            "ì¢…ëª©ì½”ë“œ": ticker,
            "ê¸°ì¤€ì¼": biz_date,
            "ì§€í‘œêµ¬ë¶„": f"{source}_E" if is_est else source,
            "ê³„ì •": account,
            "ê°’": val,
        })
    return rows


def fetch_indicators(ticker: str) -> list[dict]:
    """Financial Highlight + ì¬ë¬´ë¹„ìœ¨ + ë°°ë‹¹ê¸ˆ ìˆ˜ì§‘"""
    rows = []

    # â”€â”€ (A) ë©”ì¸ í˜ì´ì§€: Financial Highlight + DPS â”€â”€
    url_main = (
        f"https://comp.fnguide.com/SVO2/ASP/SVD_Main.asp"
        f"?pGB=1&gicode=A{ticker}&stkGb=701"
    )
    main_tables = load_tables(url_main)

    for t in main_tables:
        if not isinstance(t, pd.DataFrame) or t.shape[0] < 2 or t.shape[1] < 2:
            continue
        
        # [ìˆ˜ì •] ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜ (float/NaN ì˜¤ë¥˜ ë°©ì§€)
        col1_list = [str(x) for x in t.iloc[:, 0].values]
        col1_text = " ".join(col1_list)

        # Financial Highlight í…Œì´ë¸” ì‹ë³„
        has_rev = "ë§¤ì¶œì•¡" in col1_text or "ì˜ì—…ìˆ˜ìµ" in col1_text
        has_roe = "ROE" in col1_text
        has_op = "ì˜ì—…ì´ìµ" in col1_text
        if has_rev or has_roe or has_op:
            rows += _extract_indicator_rows(t, ticker, "HIGHLIGHT")
            break  # ì²« ë²ˆì§¸ ë§¤ì¹­ë§Œ

    # DPS (ë°°ë‹¹ê¸ˆ) â€” Highlight í…Œì´ë¸”ì—ì„œ ë³„ë„ ì¶”ì¶œ
    for t in main_tables:
        if not isinstance(t, pd.DataFrame) or t.shape[0] < 2:
            continue
        
        # [ìˆ˜ì •] ì‹œë¦¬ì¦ˆ ë³€í™˜ ì‹œì—ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        col1 = t.iloc[:, 0].astype(str)
        
        dps_idx = col1[col1.str.contains("ë°°ë‹¹ê¸ˆ|DPS", regex=True)].index
        if len(dps_idx) == 0:
            continue
        row_data = t.iloc[dps_idx[0]]
        for col_name, val in row_data.items():
            if col_name == t.columns[0]:
                continue
            biz_date, _ = parse_period(col_name)
            if biz_date is None:
                continue
            v = safe_float(val)
            if v is not None:
                rows.append({
                    "ì¢…ëª©ì½”ë“œ": ticker,
                    "ê¸°ì¤€ì¼": biz_date,
                    "ì§€í‘œêµ¬ë¶„": "DPS",
                    "ê³„ì •": "ì£¼ë‹¹ë°°ë‹¹ê¸ˆ",
                    "ê°’": v,
                })
        break

    # â”€â”€ (B) ì¬ë¬´ë¹„ìœ¨ í˜ì´ì§€ â”€â”€
    url_ratio = (
        f"https://comp.fnguide.com/SVO2/ASP/SVD_FinanceRatio.asp"
        f"?pGB=1&gicode=A{ticker}&stkGb=701"
    )
    ratio_tables = load_tables(url_ratio)
    if len(ratio_tables) >= 1:
        rows += _extract_indicator_rows(ratio_tables[0], ticker, "RATIO_Y")
    if len(ratio_tables) >= 2:
        rows += _extract_indicator_rows(ratio_tables[1], ticker, "RATIO_Q")

    return rows


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. ì£¼ì‹ìˆ˜ (FnGuide)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_shares(ticker: str) -> dict | None:
    """ë°œí–‰ì£¼ì‹ìˆ˜, ìì‚¬ì£¼, ìœ í†µì£¼ì‹ìˆ˜ ìˆ˜ì§‘"""
    url = (
        f"https://comp.fnguide.com/SVO2/ASP/SVD_Main.asp"
        f"?pGB=1&gicode=A{ticker}&stkGb=701"
    )
    tables = load_tables(url)
    if not tables:
        return None

    # ë°œí–‰ì£¼ì‹ìˆ˜
    try:
        issued = safe_int(str(tables[0].iloc[6, 1]).split("/")[0])
    except Exception:
        issued = 0

    # ìì‚¬ì£¼
    treasury = 0
    for t in tables:
        if isinstance(t, pd.DataFrame) and "ë³´í†µì£¼" in t.columns:
            try:
                val = safe_int(t["ë³´í†µì£¼"].iloc[4])
                if val is not None:
                    treasury = val
                    break
            except Exception:
                pass

    float_shares = max((issued or 0) - treasury, 0)
    return {
        "ì¢…ëª©ì½”ë“œ": ticker,
        "ê¸°ì¤€ì¼": date.today().isoformat(),
        "ë°œí–‰ì£¼ì‹ìˆ˜": issued,
        "ìì‚¬ì£¼": treasury,
        "ìœ í†µì£¼ì‹ìˆ˜": float_shares,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë³‘ë ¬ ìˆ˜ì§‘ ë˜í¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parallel_collect(func, tickers: list, desc: str) -> list:
    """ThreadPoolExecutor ë˜í¼ â€” ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:
        futures = {pool.submit(func, t): t for t in tickers}
        for f in tqdm(as_completed(futures), total=len(tickers), desc=desc):
            res = f.result()
            if res:
                if isinstance(res, list):
                    results.extend(res)
                else:
                    results.append(res)
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_crawling():
    """3ê°œ ìƒ˜í”Œ ì¢…ëª©ìœ¼ë¡œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    log.info("=" * 60)
    log.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹œì‘ (ìƒ˜í”Œ 3ê°œ ì¢…ëª©)")
    log.info("=" * 60)
    
    # ë§ˆìŠ¤í„° ë°ì´í„° ë¡œë“œ (ì¢…ëª©ëª… í™•ì¸ìš©)
    master = collect_master()
    
    test_results = {}
    
    for ticker in TEST_TICKERS:
        stock_name = master[master["ì¢…ëª©ì½”ë“œ"] == ticker]["ì¢…ëª©ëª…"].values
        stock_name = stock_name[0] if len(stock_name) > 0 else "Unknown"
        
        log.info(f"\n{'='*60}")
        log.info(f"ğŸ“Œ [{ticker}] {stock_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
        log.info(f"{'='*60}")
        
        test_results[ticker] = {
            "ì¢…ëª©ëª…": stock_name,
            "ì¬ë¬´ì œí‘œ": False,
            "ì§€í‘œ": False,
            "ì£¼ì‹ìˆ˜": False,
        }
        
        # 1) ì¬ë¬´ì œí‘œ
        try:
            fs_data = fetch_fs(ticker)
            if fs_data and len(fs_data) > 0:
                test_results[ticker]["ì¬ë¬´ì œí‘œ"] = True
                log.info(f"  âœ… ì¬ë¬´ì œí‘œ: {len(fs_data)}ê±´ ìˆ˜ì§‘ ì„±ê³µ")
                # ìƒ˜í”Œ ì¶œë ¥
                sample = pd.DataFrame(fs_data[:5])
                print(sample.to_string(index=False))
            else:
                log.warning(f"  âš ï¸  ì¬ë¬´ì œí‘œ: ë°ì´í„° ì—†ìŒ")
        except Exception as e:
            log.error(f"  âŒ ì¬ë¬´ì œí‘œ ì˜¤ë¥˜: {e}")
        
        # 2) ì§€í‘œ
        try:
            ind_data = fetch_indicators(ticker)
            if ind_data and len(ind_data) > 0:
                test_results[ticker]["ì§€í‘œ"] = True
                log.info(f"  âœ… í•µì‹¬ì§€í‘œ: {len(ind_data)}ê±´ ìˆ˜ì§‘ ì„±ê³µ")
                # ìƒ˜í”Œ ì¶œë ¥
                sample = pd.DataFrame(ind_data[:5])
                print(sample.to_string(index=False))
            else:
                log.warning(f"  âš ï¸  í•µì‹¬ì§€í‘œ: ë°ì´í„° ì—†ìŒ")
        except Exception as e:
            log.error(f"  âŒ í•µì‹¬ì§€í‘œ ì˜¤ë¥˜: {e}")
        
        # 3) ì£¼ì‹ìˆ˜
        try:
            share_data = fetch_shares(ticker)
            if share_data:
                test_results[ticker]["ì£¼ì‹ìˆ˜"] = True
                log.info(f"  âœ… ì£¼ì‹ìˆ˜: ìˆ˜ì§‘ ì„±ê³µ")
                print(f"     ë°œí–‰ì£¼ì‹ìˆ˜: {share_data['ë°œí–‰ì£¼ì‹ìˆ˜']:,}ì£¼")
                print(f"     ìì‚¬ì£¼: {share_data['ìì‚¬ì£¼']:,}ì£¼")
                print(f"     ìœ í†µì£¼ì‹ìˆ˜: {share_data['ìœ í†µì£¼ì‹ìˆ˜']:,}ì£¼")
            else:
                log.warning(f"  âš ï¸  ì£¼ì‹ìˆ˜: ë°ì´í„° ì—†ìŒ")
        except Exception as e:
            log.error(f"  âŒ ì£¼ì‹ìˆ˜ ì˜¤ë¥˜: {e}")
    
    # ê²°ê³¼ ìš”ì•½
    log.info("\n" + "=" * 60)
    log.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    log.info("=" * 60)
    
    summary_df = pd.DataFrame(test_results).T
    print(summary_df.to_string())
    
    # ì„±ê³µë¥  ê³„ì‚°
    total_tests = len(TEST_TICKERS) * 3
    passed_tests = sum([
        sum([
            test_results[t]["ì¬ë¬´ì œí‘œ"],
            test_results[t]["ì§€í‘œ"],
            test_results[t]["ì£¼ì‹ìˆ˜"]
        ])
        for t in TEST_TICKERS
    ])
    
    success_rate = (passed_tests / total_tests) * 100
    log.info(f"\nâœ… ì„±ê³µë¥ : {passed_tests}/{total_tests} ({success_rate:.1f}%)")
    
    if success_rate >= 80:
        log.info("ğŸ‰ í…ŒìŠ¤íŠ¸ í†µê³¼! ì „ì²´ ìˆ˜ì§‘ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return True
    else:
        log.warning("âš ï¸  í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ë‚˜ FnGuide ì ‘ê·¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_full():
    """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (ë‚ ì§œë³„ íŒŒì¼ ë²„ì „ ê´€ë¦¬ + ì´ì–´í•˜ê¸°)"""
    start = datetime.now()
    biz_day = get_biz_day()  # ì˜ˆ: '20260206'
    log.info(f"ğŸ“… ê¸°ì¤€ ì˜ì—…ì¼: {biz_day}")

    # â”€â”€ íŒŒì¼ëª… ì •ì˜ (ë‚ ì§œ í¬í•¨) â”€â”€
    # ì˜ˆ: master_20260206.csv
    path_master = DATA_DIR / f"master_{biz_day}.csv"
    path_daily = DATA_DIR / f"daily_{biz_day}.csv"
    path_fs = DATA_DIR / f"financial_statements_{biz_day}.csv"
    path_ind = DATA_DIR / f"indicators_{biz_day}.csv"
    path_shares = DATA_DIR / f"shares_{biz_day}.csv"

    # â”€â”€ 1) ë§ˆìŠ¤í„° â”€â”€
    if path_master.exists():
        log.info(f"ğŸ“‚ {path_master.name} íŒŒì¼ì´ ìˆì–´ ë¡œë“œí•©ë‹ˆë‹¤.")
        master = pd.read_csv(path_master, encoding="utf-8-sig")
    else:
        master = collect_master()
        master.to_csv(path_master, index=False, encoding="utf-8-sig")
        log.info(f"âœ… {path_master.name} ì €ì¥ ({len(master)}ê±´)")

    # â”€â”€ 2) ì¼ë³„ ì‹œì„¸ â”€â”€
    if path_daily.exists():
        log.info(f"ğŸ“‚ {path_daily.name} íŒŒì¼ì´ ìˆì–´ ë¡œë“œí•©ë‹ˆë‹¤.")
        daily = pd.read_csv(path_daily, encoding="utf-8-sig")
    else:
        daily = collect_daily(biz_day)
        daily.to_csv(path_daily, index=False, encoding="utf-8-sig")
        log.info(f"âœ… {path_daily.name} ì €ì¥ ({len(daily)}ê±´)")

    # ë³´í†µì£¼ë§Œ ì¶”ì¶œ (FnGuide í¬ë¡¤ë§ ëŒ€ìƒ)
    targets = master.loc[
        (master["ì¢…ëª©êµ¬ë¶„"] == "ë³´í†µì£¼") & (master["ì‹œì¥êµ¬ë¶„"].isin(["KOSPI", "KOSDAQ"])),
        "ì¢…ëª©ì½”ë“œ",
    ].tolist()
    
    # ì¢…ëª©ì½”ë“œ í¬ë§· í†µì¼ (005930)
    targets = [f"{x:06d}" if isinstance(x, (int, float)) else str(x) for x in targets]
    
    log.info(f"ğŸ¯ FnGuide í¬ë¡¤ë§ ëŒ€ìƒ: {len(targets)}ê°œ ë³´í†µì£¼")

    # â”€â”€ 3) ì¬ë¬´ì œí‘œ â”€â”€
    if path_fs.exists():
        log.info(f"â­ï¸  {path_fs.name} ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìˆ˜ì§‘ ê±´ë„ˆëœ€")
    else:
        fs_rows = parallel_collect(fetch_fs, targets, "ì¬ë¬´ì œí‘œ")
        if fs_rows:
            fs_df = pd.DataFrame(fs_rows)
            fs_df.to_csv(path_fs, index=False, encoding="utf-8-sig")
            log.info(f"âœ… {path_fs.name} ì €ì¥ ({len(fs_df)}ê±´)")
        else:
            log.warning("âš ï¸ ì¬ë¬´ì œí‘œ ë°ì´í„° ì—†ìŒ")

    # â”€â”€ 4) í•µì‹¬ ì§€í‘œ â”€â”€
    if path_ind.exists():
        log.info(f"â­ï¸  {path_ind.name} ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìˆ˜ì§‘ ê±´ë„ˆëœ€")
    else:
        ind_rows = parallel_collect(fetch_indicators, targets, "í•µì‹¬ì§€í‘œ")
        if ind_rows:
            ind_df = pd.DataFrame(ind_rows)
            ind_df.to_csv(path_ind, index=False, encoding="utf-8-sig")
            log.info(f"âœ… {path_ind.name} ì €ì¥ ({len(ind_df)}ê±´)")
        else:
            log.warning("âš ï¸ í•µì‹¬ì§€í‘œ ë°ì´í„° ì—†ìŒ")

    # â”€â”€ 5) ì£¼ì‹ìˆ˜ â”€â”€
    if path_shares.exists():
        log.info(f"â­ï¸  {path_shares.name} ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìˆ˜ì§‘ ê±´ë„ˆëœ€")
    else:
        share_rows = parallel_collect(fetch_shares, targets, "ì£¼ì‹ìˆ˜")
        if share_rows:
            share_df = pd.DataFrame(share_rows)
            share_df.to_csv(path_shares, index=False, encoding="utf-8-sig")
            log.info(f"âœ… {path_shares.name} ì €ì¥ ({len(share_df)}ê±´)")
        else:
            log.warning("âš ï¸ ì£¼ì‹ìˆ˜ ë°ì´í„° ì—†ìŒ")

    elapsed = datetime.now() - start
    log.info(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ (ì†Œìš”: {elapsed})")
    log.info(f"ğŸ“ ì €ì¥ í´ë”: {DATA_DIR.resolve()}")


def main():
    parser = argparse.ArgumentParser(
        description="í•œêµ­ ì£¼ì‹ í€€íŠ¸ ë°ì´í„° ìˆ˜ì§‘ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  í…ŒìŠ¤íŠ¸ ëª¨ë“œ (3ê°œ ì¢…ëª©):  python quant_collector.py --test
  ì „ì²´ ì‹¤í–‰:              python quant_collector.py
  í…ŒìŠ¤íŠ¸ í›„ ì „ì²´ ì‹¤í–‰:     python quant_collector.py --test --auto-proceed
        """
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ìƒ˜í”Œ 3ê°œ ì¢…ëª©ë§Œ í¬ë¡¤ë§)"
    )
    parser.add_argument(
        "--auto-proceed",
        action="store_true",
        help="í…ŒìŠ¤íŠ¸ ì„±ê³µ ì‹œ ìë™ìœ¼ë¡œ ì „ì²´ ìˆ˜ì§‘ ì§„í–‰"
    )
    
    args = parser.parse_args()
    
    if args.test:
        test_passed = test_crawling()
        
        if test_passed and args.auto_proceed:
            log.info("\nìë™ ì§„í–‰ ëª¨ë“œ: ì „ì²´ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            run_full()
        elif test_passed:
            response = input("\nì „ì²´ ìˆ˜ì§‘ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if response == 'y':
                run_full()
            else:
                log.info("ì „ì²´ ìˆ˜ì§‘ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        else:
            log.info("í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¬¸ì œë¥¼ í•´ê²° í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    else:
        run_full()


if __name__ == "__main__":
    main()
